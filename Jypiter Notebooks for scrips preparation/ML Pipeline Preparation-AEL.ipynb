{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import statements\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import random as sparse_random\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
      "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
      "       'child_alone', 'water', 'food', 'shelter', 'clothing', 'money',\n",
      "       'missing_people', 'refugees', 'death', 'other_aid',\n",
      "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
      "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
      "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
      "       'other_weather', 'direct_report'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('DisasterResponse', 'sqlite:///DisasterResponse.db')\n",
    "#df.head()\n",
    "\n",
    "# Extract messages\n",
    "X = df[['id','message','original','genre']]\n",
    "\n",
    "columns_categories = df.columns.drop(['id','message','original','genre'])\n",
    "print(columns_categories)\n",
    "\n",
    "# Extract categories columns\n",
    "Y = df[columns_categories]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products      ...        \\\n",
       "0        0      0            0             0                 0      ...         \n",
       "1        0      0            1             0                 0      ...         \n",
       "2        0      0            0             0                 0      ...         \n",
       "3        1      0            1             0                 1      ...         \n",
       "4        0      0            0             0                 0      ...         \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26216, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  \n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct  \n",
       "1                 Cyclone nan fini osinon li pa fini  direct  \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct  \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct  \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the dataframe shape\n",
    "\n",
    "print(X.shape)\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>child_alone</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "0        1        0      0            0             0                 0   \n",
       "1        1        0      0            1             0                 0   \n",
       "2        1        0      0            0             0                 0   \n",
       "3        1        1      0            1             0                 1   \n",
       "4        1        0      0            0             0                 0   \n",
       "\n",
       "   search_and_rescue  security  military  child_alone      ...        \\\n",
       "0                  0         0         0            0      ...         \n",
       "1                  0         0         0            0      ...         \n",
       "2                  0         0         0            0      ...         \n",
       "3                  0         0         0            0      ...         \n",
       "4                  0         0         0            0      ...         \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()\n",
    "#print(Y.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function obtains all the tokens from a string (text parameter) and return them in a list\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function obtain the tokens from a string\n",
    "    \n",
    "    Parameters: text is the string from the tokens are obtained\n",
    "    Return:     clean_tokens as the list of tokens \n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize text and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    #print(\"\\nTokens=\", tokens)\n",
    "    \n",
    "    # Remove stop words and lemmatize\n",
    "    #tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    \n",
    "    #print(\"\\nStop words removal and lematizer =\", clean_tokens)\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for testing the tokenize function\n",
    "\n",
    "text = \"Weather update - a cold front from Cuba that couldn't pass over Haiti\"\n",
    "#print(text)\n",
    "\n",
    "#print(tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for extracting the tokens, get the tfidf matrix and to create a multiouput classifier for training \n",
    "# a multi-levels vector y=[y1,y1,y3,...]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(max_depth=3, n_estimators= 20))) \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.message, Y)\n",
    "\n",
    "#print(X_train.shape,y_train.shape)\n",
    "\n",
    "# train classifier\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data through the previosly created pipeline\n",
    "\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the predicted values with the pipeline, and reports the scores for \n",
    "# the multiclass-multioutput-multilabel classifier: f1 score, precision and recall\n",
    "\n",
    "def display_results(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Function to display the predicted values with the pipeline, and reports the scores for the\n",
    "    multiclass-multioutput-multilabel classifier: f1 score, precision and recall\n",
    "    \n",
    "    Parameters: y_test is the vector with the testing labels\n",
    "                y_pred is the vector with the predicted labels\n",
    "    \"\"\"\n",
    "    labels = np.unique(y_pred)\n",
    "    y_test2 = np.array(y_test)\n",
    "    y_pred2 = np.array(y_pred)\n",
    "    \n",
    "    columnsTest = y_test.shape[1]\n",
    "    columnsPred = y_pred.shape[1]\n",
    "    \n",
    "    # Obtain the precision, recall, and F1 metrics for each feature in y_pred\n",
    "        \n",
    "    if columnsTest == columnsPred:\n",
    "        for i in range(columnsTest):\n",
    "            print(\"Category of message= \",y_test.columns[i])\n",
    "            print(metrics.classification_report(y_test2[:,i],y_pred2[:,i]))\n",
    "    else:\n",
    "        print(\"Columns number in y_test and y_pred are different.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category of message=  related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1497\n",
      "          1       0.76      1.00      0.87      5003\n",
      "          2       0.00      0.00      0.00        54\n",
      "\n",
      "avg / total       0.58      0.76      0.66      6554\n",
      "\n",
      "Category of message=  request\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      1.00      0.90      5392\n",
      "          1       0.00      0.00      0.00      1162\n",
      "\n",
      "avg / total       0.68      0.82      0.74      6554\n",
      "\n",
      "Category of message=  offer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      6529\n",
      "          1       0.00      0.00      0.00        25\n",
      "\n",
      "avg / total       0.99      1.00      0.99      6554\n",
      "\n",
      "Category of message=  aid_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      1.00      0.75      3876\n",
      "          1       0.89      0.02      0.03      2678\n",
      "\n",
      "avg / total       0.72      0.60      0.45      6554\n",
      "\n",
      "Category of message=  medical_help\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96      6038\n",
      "          1       0.00      0.00      0.00       516\n",
      "\n",
      "avg / total       0.85      0.92      0.88      6554\n",
      "\n",
      "Category of message=  medical_products\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      6228\n",
      "          1       0.00      0.00      0.00       326\n",
      "\n",
      "avg / total       0.90      0.95      0.93      6554\n",
      "\n",
      "Category of message=  search_and_rescue\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99      6380\n",
      "          1       0.00      0.00      0.00       174\n",
      "\n",
      "avg / total       0.95      0.97      0.96      6554\n",
      "\n",
      "Category of message=  security\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      6426\n",
      "          1       0.00      0.00      0.00       128\n",
      "\n",
      "avg / total       0.96      0.98      0.97      6554\n",
      "\n",
      "Category of message=  military\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      6350\n",
      "          1       0.00      0.00      0.00       204\n",
      "\n",
      "avg / total       0.94      0.97      0.95      6554\n",
      "\n",
      "Category of message=  child_alone\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      6554\n",
      "\n",
      "avg / total       1.00      1.00      1.00      6554\n",
      "\n",
      "Category of message=  water\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97      6137\n",
      "          1       0.00      0.00      0.00       417\n",
      "\n",
      "avg / total       0.88      0.94      0.91      6554\n",
      "\n",
      "Category of message=  food\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94      5787\n",
      "          1       0.00      0.00      0.00       767\n",
      "\n",
      "avg / total       0.78      0.88      0.83      6554\n",
      "\n",
      "Category of message=  shelter\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95      5977\n",
      "          1       0.00      0.00      0.00       577\n",
      "\n",
      "avg / total       0.83      0.91      0.87      6554\n",
      "\n",
      "Category of message=  clothing\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      6458\n",
      "          1       0.00      0.00      0.00        96\n",
      "\n",
      "avg / total       0.97      0.99      0.98      6554\n",
      "\n",
      "Category of message=  money\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      6407\n",
      "          1       0.00      0.00      0.00       147\n",
      "\n",
      "avg / total       0.96      0.98      0.97      6554\n",
      "\n",
      "Category of message=  missing_people\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      6479\n",
      "          1       0.00      0.00      0.00        75\n",
      "\n",
      "avg / total       0.98      0.99      0.98      6554\n",
      "\n",
      "Category of message=  refugees\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      6337\n",
      "          1       0.00      0.00      0.00       217\n",
      "\n",
      "avg / total       0.93      0.97      0.95      6554\n",
      "\n",
      "Category of message=  death\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      6273\n",
      "          1       0.00      0.00      0.00       281\n",
      "\n",
      "avg / total       0.92      0.96      0.94      6554\n",
      "\n",
      "Category of message=  other_aid\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93      5717\n",
      "          1       0.00      0.00      0.00       837\n",
      "\n",
      "avg / total       0.76      0.87      0.81      6554\n",
      "\n",
      "Category of message=  infrastructure_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97      6140\n",
      "          1       0.00      0.00      0.00       414\n",
      "\n",
      "avg / total       0.88      0.94      0.91      6554\n",
      "\n",
      "Category of message=  transport\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98      6243\n",
      "          1       0.00      0.00      0.00       311\n",
      "\n",
      "avg / total       0.91      0.95      0.93      6554\n",
      "\n",
      "Category of message=  buildings\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      6225\n",
      "          1       0.00      0.00      0.00       329\n",
      "\n",
      "avg / total       0.90      0.95      0.93      6554\n",
      "\n",
      "Category of message=  electricity\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      6420\n",
      "          1       0.00      0.00      0.00       134\n",
      "\n",
      "avg / total       0.96      0.98      0.97      6554\n",
      "\n",
      "Category of message=  tools\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      6517\n",
      "          1       0.00      0.00      0.00        37\n",
      "\n",
      "avg / total       0.99      0.99      0.99      6554\n",
      "\n",
      "Category of message=  hospitals\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      6489\n",
      "          1       0.00      0.00      0.00        65\n",
      "\n",
      "avg / total       0.98      0.99      0.99      6554\n",
      "\n",
      "Category of message=  shops\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      6520\n",
      "          1       0.00      0.00      0.00        34\n",
      "\n",
      "avg / total       0.99      0.99      0.99      6554\n",
      "\n",
      "Category of message=  aid_centers\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      6473\n",
      "          1       0.00      0.00      0.00        81\n",
      "\n",
      "avg / total       0.98      0.99      0.98      6554\n",
      "\n",
      "Category of message=  other_infrastructure\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      6268\n",
      "          1       0.00      0.00      0.00       286\n",
      "\n",
      "avg / total       0.91      0.96      0.94      6554\n",
      "\n",
      "Category of message=  weather_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      1.00      0.84      4769\n",
      "          1       0.80      0.00      0.00      1785\n",
      "\n",
      "avg / total       0.75      0.73      0.61      6554\n",
      "\n",
      "Category of message=  floods\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96      6033\n",
      "          1       0.00      0.00      0.00       521\n",
      "\n",
      "avg / total       0.85      0.92      0.88      6554\n",
      "\n",
      "Category of message=  storm\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95      5926\n",
      "          1       0.00      0.00      0.00       628\n",
      "\n",
      "avg / total       0.82      0.90      0.86      6554\n",
      "\n",
      "Category of message=  fire\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      6483\n",
      "          1       0.00      0.00      0.00        71\n",
      "\n",
      "avg / total       0.98      0.99      0.98      6554\n",
      "\n",
      "Category of message=  earthquake\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95      5982\n",
      "          1       0.00      0.00      0.00       572\n",
      "\n",
      "avg / total       0.83      0.91      0.87      6554\n",
      "\n",
      "Category of message=  cold\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      6430\n",
      "          1       0.00      0.00      0.00       124\n",
      "\n",
      "avg / total       0.96      0.98      0.97      6554\n",
      "\n",
      "Category of message=  other_weather\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      6221\n",
      "          1       0.00      0.00      0.00       333\n",
      "\n",
      "avg / total       0.90      0.95      0.92      6554\n",
      "\n",
      "Category of message=  direct_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      1.00      0.89      5243\n",
      "          1       0.00      0.00      0.00      1311\n",
      "\n",
      "avg / total       0.64      0.80      0.71      6554\n",
      "\n",
      "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
      "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
      "       'child_alone', 'water', 'food', 'shelter', 'clothing', 'money',\n",
      "       'missing_people', 'refugees', 'death', 'other_aid',\n",
      "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
      "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
      "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
      "       'other_weather', 'direct_report'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Display results, reporting the f1 score, precision and recall for each \n",
    "# output category of the dataset\n",
    "\n",
    "display_results(y_test, y_pred)\n",
    "print(y_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The next code defines a pipeline for extracting the tokens, get the tfidf matrix and to create a multiouput classifier for training \n",
    "# a multi-levels vector y=[y1,y1,y3,...] and test the hyperparameters with the GridSearchCV() function to find the best ones\n",
    "\n",
    "mclf = MultiOutputClassifier(RandomForestClassifier())\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('mclf', mclf) \n",
    "    ])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range':[(1,1), (1,2)], # Allow unigrams, bigrams or both.\n",
    "    'tfidf__norm':('l1', 'l2'), # Test if l1, l2 or None train better\n",
    "    \"mclf__estimator__max_depth\": [4], # Tree depth in the forest\n",
    "    \"mclf__estimator__n_estimators\": [10, 20], # Number of trees in the forest\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(estimator=pipeline, param_grid=parameters,refit=True,verbose=2,n_jobs=-1)\n",
    "\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameter (CV score=%0.3f):\" % cv.best_score_)\n",
    "print(cv.best_params_)\n",
    "\n",
    "# Make the predictions with the gridsearch pipeline\n",
    "\n",
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results, reporting the f1 score, precision and recall for each \n",
    "# output category of the dataset using the best hyperparameters which resulted from the previous classifier training\n",
    "\n",
    "display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(svd_2000.shape)\n",
    "\n",
    "#print(X_train.shape,y_train.shape)\n",
    "#print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textLenghExtractor(text):\n",
    "    \"\"\"\n",
    "    Function to calculate the lenght of an array of strings\n",
    "    \n",
    "    Parameters: text is the array of strings\n",
    "    Return:     Array of lenghts\n",
    "    \"\"\"\n",
    "    print(len(text))\n",
    "    \n",
    "    #mylen = np.empty([len(text)], dtype=int)\n",
    "    #print(\"len 1\", mylen)\n",
    "    \n",
    "    mylen = np.vectorize(len)(text)\n",
    "    \n",
    "    print(\"mylen\", mylen.shape)\n",
    "    \n",
    "    mylen2 = mylen.reshape(-1,1)\n",
    "    print(\"mylen2\", mylen2)\n",
    "    print(\"mylen2 shape\", mylen2.shape)\n",
    "    \n",
    "    return mylen2\n",
    "\n",
    "#arr = np.array(['Hello', 'foo', 'and', 'whatsoever']) \n",
    "\n",
    "#print(textLenghExtractor(arr))\n",
    "\n",
    "#messagesLens = FunctionTransformer(textLenghExtractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following class defines and extractor for calculating the lenghts of a strings array.\n",
    "\n",
    "class TextLenghExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Class for defining an extractor for calculating the lenghts of a strings array\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Function for fitting the transformer\n",
    "\n",
    "        Parameters: self is the same extractor\n",
    "                    X is an array of strings\n",
    "        Return:     The extractor\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, text):\n",
    "        \"\"\"\n",
    "        Function for calculating the lenghts over and array of strings\n",
    "\n",
    "        Parameters: self is the same extractor\n",
    "                    X is an 2D array\n",
    "        Return:     The extractor\n",
    "        \"\"\"\n",
    "        #print(text.shape)\n",
    "        lens = []\n",
    "        for x in text:\n",
    "            lens.append(len(x))\n",
    "            #print(len(x), lens)\n",
    "        lengths = np.asarray(lens).reshape(-1,1)\n",
    "        return lengths\n",
    "\n",
    "textlen = TextLenghExtractor()\n",
    "#textlen.fit(X_train)\n",
    "\n",
    "#lengs = textlen.transform(X_train)\n",
    "\n",
    "#print(lengs.shape)\n",
    "#print(len(lengs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following defines a feature union transformer with a pipeline for obtaining the tokens and the tfidf, \n",
    "# then to calculate the lenghts of the original X_train, in parallel\n",
    "\n",
    "fu = FeatureUnion([ \n",
    "               ('nlp_pipeline', Pipeline([ ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                            ('tfidf', TfidfTransformer()) ]) ), \n",
    "               ('textlen', textlen )\n",
    "             ]) #End of duplas list for FeatureUnion\n",
    "fu.fit(X_train)\n",
    "x = fu.transform(X_train)\n",
    "\n",
    "#print(\"feature union= \",x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] estimator=MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "           max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
      "           n_jobs=1), estimator__estimator__n_estimators=10, features__nlp_pipeline__tfidf__norm=l2, features__nlp_pipeline__vect__ngram_range=(1, 2) \n",
      "[CV]  estimator=MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "           max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
      "           n_jobs=1), estimator__estimator__n_estimators=10, features__nlp_pipeline__tfidf__norm=l2, features__nlp_pipeline__vect__ngram_range=(1, 2), total=  20.4s\n",
      "[CV] estimator=MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "           max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
      "           n_jobs=1), estimator__estimator__n_estimators=10, features__nlp_pipeline__tfidf__norm=l2, features__nlp_pipeline__vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   32.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator=MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "           max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
      "           n_jobs=1), estimator__estimator__n_estimators=10, features__nlp_pipeline__tfidf__norm=l2, features__nlp_pipeline__vect__ngram_range=(1, 2), total=  20.4s\n",
      "[CV] estimator=MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "           max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
      "           n_jobs=1), estimator__estimator__n_estimators=10, features__nlp_pipeline__tfidf__norm=l2, features__nlp_pipeline__vect__ngram_range=(1, 2) \n",
      "[CV]  estimator=MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "           max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
      "           n_jobs=1), estimator__estimator__n_estimators=10, features__nlp_pipeline__tfidf__norm=l2, features__nlp_pipeline__vect__ngram_range=(1, 2), total=  20.8s\n",
      "[CV] estimator=MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "           n_jobs=1), estimator__estimator__n_estimators=8, features__nlp_pipeline__tfidf__norm=l2, features__nlp_pipeline__vect__ngram_range=(1, 2) \n",
      "[CV]  estimator=MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "           n_jobs=1), estimator__estimator__n_estimators=8, features__nlp_pipeline__tfidf__norm=l2, features__nlp_pipeline__vect__ngram_range=(1, 2), total= 1.5min\n",
      "[CV] estimator=MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=8, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "           n_jobs=1), estimator__estimator__n_estimators=8, features__nlp_pipeline__tfidf__norm=l2, features__nlp_pipeline__vect__ngram_range=(1, 2) \n",
      "[CV]  estimator=MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=8, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "           n_jobs=1), estimator__estimator__n_estimators=8, features__nlp_pipeline__tfidf__norm=l2, features__nlp_pipeline__vect__ngram_range=(1, 2), total= 1.5min\n",
      "[CV] estimator=MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=8, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "           n_jobs=1), estimator__estimator__n_estimators=8, features__nlp_pipeline__tfidf__norm=l2, features__nlp_pipeline__vect__ngram_range=(1, 2) \n",
      "[CV]  estimator=MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=8, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "           n_jobs=1), estimator__estimator__n_estimators=8, features__nlp_pipeline__tfidf__norm=l2, features__nlp_pipeline__vect__ngram_range=(1, 2), total= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('nlp_pipeline', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df...=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'features__nlp_pipeline__vect__ngram_range': [(1, 2)], 'features__nlp_pipeline__tfidf__norm': ['l2'], 'estimator': [MultiOutputClassifier(estimator=ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "           max_depth=3, max_features='auto', max_leaf_nodes=None,...0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1)], 'estimator__estimator__n_estimators': [8]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The next code defines a pipeline for extracting the tokens, get the tfidf matrix and to create a multiouput classifier for training \n",
    "# a multi-levels vector y=[y1,y1,y3,...] and test the hyperparameters with the GridSearchCV() function to find the best ones\n",
    "# then, it tests two different classifiers and different hyperparamenters for each of them, and finds the best ones\n",
    "\n",
    "pipeline3 = Pipeline([\n",
    "            ('features', FeatureUnion([ \n",
    "               ('nlp_pipeline', Pipeline([ ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                            ('tfidf', TfidfTransformer()) ]) ), \n",
    "               ('textlen', textlen ),\n",
    "                ]) #End of duplas list for FeatureUnion\n",
    "            ),\n",
    "            ('estimator', MultiOutputClassifier(ExtraTreesClassifier(random_state=0, bootstrap=True, max_depth=3))),\n",
    "            #('estimator', MultiOutputClassifier(RandomForestClassifier())),\n",
    "            \n",
    "            ])\n",
    "\n",
    "#pipeline3.fit(X_train, y_train)\n",
    "\n",
    "parameters = [\n",
    "                {\n",
    "                'features__nlp_pipeline__vect__ngram_range':[(1,2)],          # Allow unigrams, bigrams or both.\n",
    "                'features__nlp_pipeline__tfidf__norm':['l2'],                 # Test if l1, l2 or None train better\n",
    "                'estimator':[MultiOutputClassifier(ExtraTreesClassifier(random_state=0, bootstrap=True, max_depth=3))],\n",
    "                'estimator__estimator__n_estimators': [10],\n",
    "                },\n",
    "                 {\n",
    "                'features__nlp_pipeline__vect__ngram_range':[(1,2)],          # Allow unigrams, bigrams or both.\n",
    "                'features__nlp_pipeline__tfidf__norm':['l2'],                 # Test if l1, l2 or None train better\n",
    "                'estimator':[MultiOutputClassifier(RandomForestClassifier())],\n",
    "                'estimator__estimator__n_estimators': [8],\n",
    "                }\n",
    "                #{\n",
    "                #'features__nlp_pipeline__vect__ngram_range':[(1,2)],          # Allow unigrams, bigrams or both.\n",
    "                #'features__nlp_pipeline__tfidf__norm':['l2'],                 # Test if l1, l2 or None train better\n",
    "                #'estimator':[MultiOutputClassifier(KNeighborsClassifier())],\n",
    "                #'estimator__estimator__n_neighbors': [3],\n",
    "                #}\n",
    "            ]\n",
    "\n",
    "        \n",
    "cv = GridSearchCV(estimator=pipeline3, param_grid=parameters,refit=True,verbose=2,n_jobs=-1)\n",
    "\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the best hyperparameters results\n",
    "print(\"Best parameter (CV score=%0.3f):\" % cv.best_score_)\n",
    "print(cv.best_params_)\n",
    "\n",
    "# Make the prediction for the testing data\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "# Display results, reporting the f1 score, precision and recall for each \n",
    "# output category of the dataset\n",
    "display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the model to a file\n",
    "\n",
    "pickle.dump(cv, open('BestModelAndGridSearch.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
